{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Projection-as-a-Layer\n",
    "This notebook implements projection to the simplex as a differentiable (?) layer.\n",
    "The algorithm implemented was described in:\n",
    "    \" Efficient projections to the ell_1 ball for Learning in High Dimensions \"\n",
    "    by Duchi et al\n",
    "and also in [this note](https://arxiv.org/pdf/1309.1541.pdf)\n",
    "\n",
    "The code in this notebook draws heavily (some might even say copies) the code freely available [here](https://github.com/smatmo/ProjectionOntoSimplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from FPN import FPN\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # Weird hack I needed on my machine as PyTorch was crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VINet(FPN):\n",
    "#     '''\n",
    "#         Test implementation of variational inequality net for learning to\n",
    "#         solve a VI over the probability simplex.\n",
    "        \n",
    "#         WARNING: This code is, as yet, untested.\n",
    "        \n",
    "#         Daniel McKenzie, April 20th 2021\n",
    "        \n",
    "#     '''\n",
    "#     def __init__(self, action_dim, num_players, device,\n",
    "#                  s_hi=1.0, inf_dim=10):\n",
    "#         super().__init__()\n",
    "#         self._device = device\n",
    "#         self._lat_dim = action_dim*num_players\n",
    "#         self._inf_dim = inf_dim\n",
    "#         self._device = device\n",
    "        \n",
    "#         # Layers\n",
    "#         self.fc_u = nn.Linear(lat_dim, lat_dim, bias=False)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#     def name(self):\n",
    "#         return 'VINet'\n",
    "        \n",
    "#     def device(self):\n",
    "#         return self._device\n",
    "    \n",
    "#     def lat_dim(self):\n",
    "#         return self._lat_dim\n",
    "    \n",
    "#     def s_hi(self):\n",
    "#         return self._s_hi\n",
    "    \n",
    "#     def project_to_simplex(self, u):\n",
    "#        \"\"\"\n",
    "#            function handling the projection to simplex.\n",
    "#        \"\"\"\n",
    "#        batch_size = u.shape[0]\n",
    "#        mu = torch.sort(u, descending=True)[0]\n",
    "#        cum_sum = torch.cumsum(mu, dim=1)\n",
    "#        # Don't actually need to track gradients in next step:\n",
    "#        j = torch.unsqueeze(torch.arange(1,self._lat_dim + 1,\n",
    "#                           dtype = mu.dtype, device = self._device),0)\n",
    "#        rho = torch.sum(j*mu - cum_sum + 1. > 0.0,dim=1, keepdim=True) - 1.\n",
    "#        rho = rho.long()\n",
    "#        sum_to_rho = cum_sum[torch.arange(batch_size), rho[:,0]]\n",
    "#        theta = (1 - torch.unsqueeze(sum_to_rho, -1))/(rho.type(sum_to_rho.dtype) + 1)\n",
    "#        w = torch.clamp(theta + u, min=0.0)\n",
    "#        return w\n",
    "    \n",
    "#     def latent_space_forward(self, u, v):\n",
    "#         u = 0.99*self.relu(self.fc_u(u) + v)\n",
    "        \n",
    "#         # Now do projection on to simplex\n",
    "        \n",
    "#         w = self.project_to_simplex(u)\n",
    "        \n",
    "#         return w\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "u = torch.tensor(np.random.randn(64,20), requires_grad=True)\n",
    "batch_size = u.shape[0]\n",
    "lat_dim = u.shape[1]\n",
    "\n",
    "def project_simplex(u, lat_dim, device):\n",
    "        \"\"\"\n",
    "            function handling the projection to simplex.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = u.shape[0]\n",
    "        mu = torch.sort(u, descending=True)[0]\n",
    "        cum_sum = torch.cumsum(mu, dim=1)\n",
    "        # Don't actually need to track gradients in next step:\n",
    "        j = torch.unsqueeze(torch.arange(1,lat_dim + 1,\n",
    "                           dtype = mu.dtype, device = device),0)\n",
    "        rho = torch.sum(j*mu - cum_sum + 1. > 0.0,dim=1, keepdim=True) - 1.\n",
    "        rho = rho.long()\n",
    "        sum_to_rho = cum_sum[torch.arange(batch_size), rho[:,0]]\n",
    "        theta = (1 - torch.unsqueeze(sum_to_rho, -1))/(rho.type(sum_to_rho.dtype) + 1)\n",
    "        w = torch.clamp(theta + u, min=0.0)\n",
    "        return w\n",
    "\n",
    "    \n",
    "# Testing that Torch is tracking the gradients properly. It seems to be working!\n",
    "w = project_simplex(u, lat_dim, device)\n",
    "print(torch.sum(w))\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999998211860657\n",
      "tensor([0.0430, 0.0000, 0.1703, 0.2506, 0.2973, 0.0000, 0.0000, 0.2387, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Test the projection function\n",
    "Data = torch.rand(128,20)\n",
    "FixedPoints = project_simplex(torch.rand(128, 10), 10, device = \"cpu\")\n",
    "\n",
    "print(torch.sum(FixedPoints[0,:]).item())\n",
    "print(FixedPoints[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very simple Test test NN that uses projection to the simplex as its final layer.\n",
    "# Note that this is an explicit, not implicit model! \n",
    "# Just wanted to test that we can back-prop through the projection layer.\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Two layer network for testing\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self._data_dim = data_dim\n",
    "        self._lat_dim = latent_dim\n",
    "        self._device = \"cpu\"\n",
    "        self.fc1 = nn.Linear(in_features = self._data_dim,\n",
    "                             out_features = self._lat_dim, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features = self._lat_dim,\n",
    "                             out_features = self._lat_dim, bias = False)\n",
    "        \n",
    "    def project_to_simplex(self, u):\n",
    "       \"\"\"\n",
    "           function handling the projection to simplex.\n",
    "       \"\"\"\n",
    "       batch_size = u.shape[0]\n",
    "       mu = torch.sort(u, descending=True)[0]\n",
    "       cum_sum = torch.cumsum(mu, dim=1)\n",
    "       # Don't actually need to track gradients in next step:\n",
    "       j = torch.unsqueeze(torch.arange(1,self._lat_dim + 1,\n",
    "                          dtype = mu.dtype, device = self._device),0)\n",
    "       rho = torch.sum(j*mu - cum_sum + 1. > 0.0,dim=1, keepdim=True) - 1.\n",
    "       rho = rho.long()\n",
    "       sum_to_rho = cum_sum[torch.arange(batch_size), rho[:,0]]\n",
    "       theta = (1 - torch.unsqueeze(sum_to_rho, -1))/(rho.type(sum_to_rho.dtype) + 1)\n",
    "       w = torch.clamp(theta + u, min=0.0)\n",
    "       return w\n",
    "        \n",
    "    def forward(self, u):\n",
    "        u = self.fc1(u)\n",
    "        u = self.relu(u)\n",
    "        u = self.fc2(u)\n",
    "        w = self.project_to_simplex(u)\n",
    "        return w\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=20, out_features=10, bias=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=False)\n",
      ")\n",
      "0.025717373937368393\n",
      "0.023060040548443794\n",
      "0.021088670939207077\n",
      "0.01971374824643135\n",
      "0.01877906545996666\n",
      "0.018249332904815674\n",
      "0.017911424860358238\n",
      "0.01774100959300995\n",
      "0.017667662352323532\n",
      "0.017655083909630775\n",
      "0.017659761011600494\n",
      "0.017658229917287827\n",
      "0.01765677146613598\n",
      "0.017650645226240158\n",
      "0.0176463071256876\n",
      "0.017657436430454254\n",
      "0.017664175480604172\n",
      "0.017684290185570717\n",
      "0.017722895368933678\n",
      "0.01776299439370632\n"
     ]
    }
   ],
   "source": [
    "# Testing training SimpleNet. The Data and labels (in this case, target fixed points) are randomly generated.\n",
    "# Seems to work just fine, although it does learn to predict the uniform distribution given any input data,\n",
    "# but perhaps this is the correct answer for random data?\n",
    "\n",
    "Data = torch.rand(128,20)\n",
    "FixedPoints = project_simplex(torch.rand(128, 10), 10, device = \"cpu\")\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "SimpleNet1 = SimpleNet(20, 10)\n",
    "print(SimpleNet1)\n",
    "optimizer = optim.Adam(SimpleNet1.parameters(),lr=1e-2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    prediction = SimpleNet1(Data)\n",
    "    loss = loss_func(prediction, FixedPoints)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], grad_fn=<SliceBackward>)\n",
      "tensor([0.0198, 0.0996, 0.1261, 0.1039, 0.0000, 0.1828, 0.0000, 0.2279, 0.0326,\n",
      "        0.2073], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test a few of simple net's predictions:\n",
    "print(prediction[1,:])\n",
    "print(prediction[8,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
